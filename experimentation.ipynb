{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pocket text classification\n",
    "\n",
    "This project aims to, somehow, automatically create tags for Pocket articles. \n",
    "It considers your currently tagged Pocket articles to determine new tags.\n",
    "\n",
    "## Getting content from Pocket\n",
    "\n",
    "I've already done it, so I won't cover it right now... however, I'll add it here when possible.\n",
    "\n",
    "## Preparing our data\n",
    "\n",
    "We have two (real) sets of data: tagged (manually) and untagged articles. These articles are of many kinds, which I usually read (or queue for eternity) on the commute to work. \n",
    "\n",
    "I used to tag them, but it became really hard to catch up, so I decided to do it automatically somehow.\n",
    "\n",
    "We want to find out the correlation between the article's text (which we acquired in [Getting content from Pocket](#Getting-content-from-Pocket) and the tags I added to them. We want to find features, or stuff that seems to be the relation between article and tag (words that define the content of the article, in a way).\n",
    "\n",
    "### Loading and playing with our data set\n",
    "\n",
    "Opening the json file (```'item_id', 'resolved_url', 'tags'``` are from the Pocket API, `'text'` comes from running BeautifulSoup on all articles, mentioned on [Getting content from Pocket](#Getting-content-from-Pocket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fd = open(\"tagged_text.json\")\n",
    "tagged_text_list = json.loads(fd.read())\n",
    "# dict_keys(['item_id', 'resolved_url', 'tags', 'text'])\n",
    "print(tagged_text_list[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download, word_tokenize\n",
    "download('punkt')\n",
    "\n",
    "test_text = tagged_text_list[0].get('text')\n",
    "tokens = word_tokenize(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the most common words from our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(word.lower() for word in tokens)\n",
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the least common words from our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(100)[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used so jupyter can plot\n",
    "%matplotlib inline\n",
    "fdist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First possibility: get outstanding words in relation to all articles\n",
    "\n",
    "One possibility is to try to separate common words from outstanding words.\n",
    "\n",
    "We can tokenize all the articles and put them in the same pool of words. Then, we count how many times a word is repeated in all pool of tokens.\n",
    "\n",
    "Common words, such as connectors, nouns, verbs, will probably be very common. In other hand, specific words will repeat less in the pool of tokens (they may repeat a lot in a single article).\n",
    "\n",
    "In other words, we want to find words in an article that have small probability to be found in other articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
