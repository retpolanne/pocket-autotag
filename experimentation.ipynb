{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pocket text classification\n",
    "\n",
    "This project aims to, somehow, automatically create tags for Pocket articles. \n",
    "It considers your currently tagged Pocket articles to determine new tags.\n",
    "\n",
    "## Getting content from Pocket\n",
    "\n",
    "I've already done it, so I won't cover it right now... however, I'll add it here when possible.\n",
    "\n",
    "## Preparing our data\n",
    "\n",
    "We have two (real) sets of data: tagged (manually) and untagged articles. These articles are of many kinds, which I usually read (or queue for eternity) on the commute to work. \n",
    "\n",
    "I used to tag them, but it became really hard to catch up, so I decided to do it automatically somehow.\n",
    "\n",
    "We want to find out the correlation between the article's text (which we acquired in [Getting content from Pocket](#Getting-content-from-Pocket) and the tags I added to them. We want to find features, or stuff that seems to be the relation between article and tag (words that define the content of the article, in a way).\n",
    "\n",
    "### Loading and playing with our data set\n",
    "\n",
    "Opening the json file (```'item_id', 'resolved_url', 'tags'``` are from the Pocket API, `'text'` comes from running BeautifulSoup on all articles, mentioned on [Getting content from Pocket](#Getting-content-from-Pocket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fd = open(\"tagged_text.json\")\n",
    "tagged_text_list = json.loads(fd.read())\n",
    "print(tagged_text_list[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download, word_tokenize\n",
    "download('punkt')\n",
    "\n",
    "test_text = tagged_text_list[0].get('text')\n",
    "tokens = word_tokenize(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the most common words from our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(word.lower() for word in tokens)\n",
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the least common words from our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(100)[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used so jupyter can plot\n",
    "%matplotlib inline\n",
    "fdist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First possibility: get outstanding words in relation to all articles\n",
    "\n",
    "One possibility is to try to separate common words from outstanding words.\n",
    "\n",
    "We can tokenize all the articles and put them in the same pool of words. Then, we count how many times a word is repeated in all pool of tokens.\n",
    "\n",
    "Common words, such as connectors, nouns, verbs, will probably be very common. In other hand, specific words will repeat less in the pool of tokens (they may repeat a lot in a single article).\n",
    "\n",
    "In other words, we want to find words in an article that have small probability to be found in other articles.\n",
    "\n",
    "First of all, I'll grab all the tokens from a text. Next, I'll grab only the tokens and add them to an array of token occurences (i.e. how many articles does this token appear). It will hold content such as:\n",
    "\n",
    "```json\n",
    "# token, occurences\n",
    "{\"word\": 2}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "token_occurences = {}\n",
    "token_probabilities = {}\n",
    "# We just want to text the 3 first articles right now\n",
    "# tagged_text_list = [text for text in tagged_text_list[0:3]]\n",
    "# Getting all tokens occurences across texts\n",
    "for article in tagged_text_list:\n",
    "    content = article.get('text')\n",
    "    tokens = word_tokenize(content)\n",
    "    fdist = FreqDist(word.lower() for word in tokens)\n",
    "    for word in fdist.most_common(100):\n",
    "        if word[0] in token_occurences.keys():\n",
    "            token_occurences[word[0]] = token_occurences[word[0]] + 1\n",
    "        else:\n",
    "            token_occurences[word[0]] = 1\n",
    "\n",
    "# Calculating token probabilities\n",
    "\n",
    "for token, count in token_occurences.items():\n",
    "    token_probabilities[token] = count / len(tagged_text_list) * 100\n",
    "\n",
    "# Getting words with less than 50% of probability\n",
    "\n",
    "filtered_dict = {}\n",
    "threshold = 50\n",
    "for key, value in token_probabilities.items():\n",
    "    if value <= threshold:\n",
    "        filtered_dict[key] = value\n",
    "\n",
    "print(filtered_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to see which most common words in an article show as less common words in our word pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_features = []\n",
    "article = tagged_text_list[0]\n",
    "content = article.get('text')\n",
    "tokens = word_tokenize(content)\n",
    "fdist = FreqDist(word.lower() for word in tokens)\n",
    "for word in fdist.most_common(100):\n",
    "    if word[0] in filtered_dict.keys():\n",
    "        if word[0] not in article_features:\n",
    "            article_features.append(word[0])\n",
    "print(article_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
